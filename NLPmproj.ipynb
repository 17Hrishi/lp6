{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP MINI PROJECT"
      ],
      "metadata": {
        "id": "OQqntDAH1eUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Title: Feature Extraction using seven moment variants"
      ],
      "metadata": {
        "id": "rTXDDDnt1fRL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUHC8SG6z8hJ",
        "outputId": "dd4623d3-d563-4aab-9b1f-861f5f0ad223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install spacy nltk scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "from nltk import ne_chunk\n",
        "from nltk.tree import Tree\n",
        "from statistics import mean"
      ],
      "metadata": {
        "id": "hAclekeQz-5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Load SpaCy's pre-trained model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEnHEP1Bz-8K",
        "outputId": "e60f5cc4-eb24-4fe3-ce06-c71d378dc0ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "    return tokens, sent_tokenize(text)\n",
        "\n",
        "def extract_word_frequency(tokens):\n",
        "    word_freq = Counter(tokens)\n",
        "    return word_freq\n",
        "\n",
        "def extract_tfidf(texts):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    return tfidf_matrix.sum(axis=0).A1\n",
        "\n",
        "def extract_pos_tag_distribution(tokens):\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    pos_count = Counter(tag for word, tag in pos_tags)\n",
        "    total_tags = sum(pos_count.values())\n",
        "    pos_distribution = {tag: count / total_tags for tag, count in pos_count.items()}\n",
        "    return pos_distribution\n",
        "\n",
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.label_ for ent in doc.ents]\n",
        "    entity_count = Counter(entities)\n",
        "    return entity_count\n",
        "\n",
        "def extract_sentence_length_statistics(sentences):\n",
        "    sentence_lengths = [len(word_tokenize(sentence)) for sentence in sentences]\n",
        "    avg_sentence_length = mean(sentence_lengths)\n",
        "    return avg_sentence_length\n",
        "\n",
        "def extract_lexical_diversity(tokens):\n",
        "    unique_words = set(tokens)\n",
        "    ttr = len(unique_words) / len(tokens) if len(tokens) > 0 else 0\n",
        "    return ttr\n",
        "\n",
        "def extract_syntactic_parse_depth(text):\n",
        "    doc = nlp(text)\n",
        "    parse_depths = [len(list(token.subtree)) for token in doc if token.dep_ == 'punct']\n",
        "    avg_parse_depth = mean(parse_depths) if parse_depths else 0\n",
        "    return avg_parse_depth\n",
        "\n",
        "def extract_features(text):\n",
        "    tokens, sentences = preprocess_text(text)\n",
        "\n",
        "    # Feature 1: Word Frequency\n",
        "    word_frequency = extract_word_frequency(tokens)\n",
        "\n",
        "    # Feature 2: TF-IDF\n",
        "    tfidf_scores = extract_tfidf([text])\n",
        "\n",
        "    # Feature 3: POS Tag Distribution\n",
        "    pos_distribution = extract_pos_tag_distribution(tokens)\n",
        "\n",
        "    # Feature 4: Named Entities\n",
        "    named_entities = extract_named_entities(text)\n",
        "\n",
        "    # Feature 5: Sentence Length Statistics\n",
        "    avg_sentence_length = extract_sentence_length_statistics(sentences)\n",
        "\n",
        "    # Feature 6: Lexical Diversity (Type-Token Ratio)\n",
        "    lexical_diversity = extract_lexical_diversity(tokens)\n",
        "\n",
        "    # Feature 7: Syntactic Parse Depth\n",
        "    avg_parse_depth = extract_syntactic_parse_depth(text)\n",
        "\n",
        "    features = {\n",
        "        \"word_frequency\": word_frequency,\n",
        "        \"tfidf_scores\": tfidf_scores,\n",
        "        \"pos_distribution\": pos_distribution,\n",
        "        \"named_entities\": named_entities,\n",
        "        \"avg_sentence_length\": avg_sentence_length,\n",
        "        \"lexical_diversity\": lexical_diversity,\n",
        "        \"avg_parse_depth\": avg_parse_depth\n",
        "    }\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "vzBh2vNnz_Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and human language.\n",
        "It enables machines to understand, interpret, and generate human language. NLP is used in many applications like chatbots, translation services, and sentiment analysis.\n",
        "\"\"\"\n",
        "\n",
        "features = extract_features(text)\n",
        "\n",
        "# Display extracted features\n",
        "for feature_name, feature_value in features.items():\n",
        "    print(f\"\\n{feature_name}: \\n{feature_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4aOeicuz_C7",
        "outputId": "cd15c2e8-0bd5-4609-c472-c30e92b92b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "word_frequency: \n",
            "Counter({'language': 3, 'nlp': 2, 'human': 2, 'natural': 1, 'processing': 1, 'branch': 1, 'artificial': 1, 'intelligence': 1, 'focuses': 1, 'interaction': 1, 'computers': 1, 'enables': 1, 'machines': 1, 'understand': 1, 'interpret': 1, 'generate': 1, 'used': 1, 'many': 1, 'applications': 1, 'like': 1, 'chatbots': 1, 'translation': 1, 'services': 1, 'sentiment': 1, 'analysis': 1})\n",
            "\n",
            "tfidf_scores: \n",
            "[0.12909944 0.38729833 0.12909944 0.12909944 0.12909944 0.12909944\n",
            " 0.12909944 0.12909944 0.12909944 0.12909944 0.12909944 0.25819889\n",
            " 0.12909944 0.12909944 0.12909944 0.12909944 0.25819889 0.12909944\n",
            " 0.38729833 0.12909944 0.12909944 0.12909944 0.12909944 0.25819889\n",
            " 0.12909944 0.12909944 0.12909944 0.12909944 0.12909944 0.12909944\n",
            " 0.12909944 0.12909944 0.12909944 0.12909944 0.12909944]\n",
            "\n",
            "pos_distribution: \n",
            "{'JJ': 0.20689655172413793, 'NN': 0.4827586206896552, 'VBZ': 0.06896551724137931, 'NNS': 0.1724137931034483, 'VBN': 0.034482758620689655, 'IN': 0.034482758620689655}\n",
            "\n",
            "named_entities: \n",
            "Counter({'ORG': 2})\n",
            "\n",
            "avg_sentence_length: \n",
            "17.333333333333332\n",
            "\n",
            "lexical_diversity: \n",
            "0.8620689655172413\n",
            "\n",
            "avg_parse_depth: \n",
            "1.2222222222222223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Completed"
      ],
      "metadata": {
        "id": "K1qiCmwZ1aQE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ygpmQSR_1cit"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}