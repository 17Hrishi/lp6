{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BNVplg6XThcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Uz26T1a6Thfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "mgQg2jNxTgCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"\\nüëâ Before Positional Encoding:\\n\", x[0][:5])\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        print(\"\\n‚úÖ After Positional Encoding:\\n\", x[0][:5])\n",
        "        return x"
      ],
      "metadata": {
        "id": "CWO0asb3TgFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaled Dot-Product Attention\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    attn = F.softmax(scores, dim=-1)\n",
        "    return torch.matmul(attn, value), attn"
      ],
      "metadata": {
        "id": "B8mXoD7JTgHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.d_k = embed_dim // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.linear_q = nn.Linear(embed_dim, embed_dim)\n",
        "        self.linear_k = nn.Linear(embed_dim, embed_dim)\n",
        "        self.linear_v = nn.Linear(embed_dim, embed_dim)\n",
        "        self.linear_out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        query = self.linear_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "        key   = self.linear_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "        value = self.linear_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "        attn_output, attn_weights = scaled_dot_product_attention(query, key, value, mask)\n",
        "        print(\"\\nüîç Attention Output (head 0):\\n\", attn_output[0, 0, :5])\n",
        "\n",
        "        x = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "        return self.linear_out(x)"
      ],
      "metadata": {
        "id": "2Q5hsYF8TgKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feedforward Layer\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.fc2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"\\n‚öôÔ∏è Before FeedForward:\\n\", x[0][:5])\n",
        "        x = self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
        "        print(\"\\n‚úÖ After FeedForward:\\n\", x[0][:5])\n",
        "        return x"
      ],
      "metadata": {
        "id": "Fbh8VK80TgMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.ff = PositionwiseFeedForward(embed_dim, ff_dim, dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, mask=None):\n",
        "        print(\"\\nüß† === Transformer Encoder Layer ===\")\n",
        "        attn_output = self.self_attn(src, src, src, mask)\n",
        "        src = self.norm1(src + self.dropout(attn_output))\n",
        "        ff_output = self.ff(src)\n",
        "        src = self.norm2(src + self.dropout(ff_output))\n",
        "        return src"
      ],
      "metadata": {
        "id": "pzeM4Re5TgO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Transformer Encoder\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len=100):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoder = PositionalEncoding(embed_dim, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_dim, num_heads, ff_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.output = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, src, mask=None):\n",
        "        x = self.embed(src)\n",
        "        print(\"\\nüì¶ Embeddings:\\n\", x[0][:5])\n",
        "        x = self.pos_encoder(x)\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            print(f\"\\nüöÄ Running Encoder Layer {i+1}\")\n",
        "            x = layer(x, mask)\n",
        "        out = self.output(x)\n",
        "        print(\"\\nüì§ Final Output (Logits):\\n\", out[0][:5])\n",
        "        return out"
      ],
      "metadata": {
        "id": "E9K9j5zDTgRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model and input\n",
        "vocab_size = 50\n",
        "seq_len = 5\n",
        "embed_dim = 16\n",
        "num_heads = 4\n",
        "ff_dim = 8\n",
        "num_layers = 2\n",
        "\n",
        "model = TransformerEncoder(vocab_size, embed_dim, num_heads, ff_dim, num_layers)\n",
        "dummy_input = torch.randint(0, vocab_size, (1, seq_len))  # batch=1\n",
        "print(\"\\nüß™ Dummy Input Tokens:\\n\", dummy_input,\"\\n\")\n",
        "\n",
        "# Forward pass\n",
        "output = model(dummy_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyhIzdwhTgV4",
        "outputId": "40470994-b93f-4175-d87e-026d22f9ea88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß™ Dummy Input Tokens:\n",
            " tensor([[43, 49,  6, 31,  1]]) \n",
            "\n",
            "\n",
            "üì¶ Embeddings:\n",
            " tensor([[ 0.4547, -0.8397, -0.5690, -1.3350, -1.1348, -0.0112,  0.1043,  0.8466,\n",
            "          0.4567, -1.4613, -1.2593,  0.4286, -0.8767,  0.0951, -0.4823, -1.1379],\n",
            "        [-0.8229, -0.4861,  0.7673,  1.6116, -1.8941,  0.4194, -1.1209, -0.0390,\n",
            "          0.6138, -1.0109, -0.0146, -0.4556, -0.0081,  0.9316, -0.7015,  0.2700],\n",
            "        [-0.2373,  0.6944, -0.2046,  2.2474, -1.1990,  0.0562, -0.6037,  0.1470,\n",
            "         -1.7067, -0.3040,  0.8566, -1.1573,  0.0219, -0.5648, -0.2980, -0.8879],\n",
            "        [-0.2884, -0.5477,  1.5881, -0.5193, -0.1374, -0.5210,  0.9926,  2.0067,\n",
            "          0.1858,  0.6579, -0.6036, -0.9532, -0.2104,  0.8340,  0.9779, -0.4513],\n",
            "        [ 1.5774, -0.0252,  0.4255,  1.3154, -0.6366, -1.0235, -1.1520, -0.9622,\n",
            "          0.5502, -0.7158,  1.4739,  1.2995, -0.8994, -0.8676,  0.6065,  1.1507]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "üëâ Before Positional Encoding:\n",
            " tensor([[ 0.4547, -0.8397, -0.5690, -1.3350, -1.1348, -0.0112,  0.1043,  0.8466,\n",
            "          0.4567, -1.4613, -1.2593,  0.4286, -0.8767,  0.0951, -0.4823, -1.1379],\n",
            "        [-0.8229, -0.4861,  0.7673,  1.6116, -1.8941,  0.4194, -1.1209, -0.0390,\n",
            "          0.6138, -1.0109, -0.0146, -0.4556, -0.0081,  0.9316, -0.7015,  0.2700],\n",
            "        [-0.2373,  0.6944, -0.2046,  2.2474, -1.1990,  0.0562, -0.6037,  0.1470,\n",
            "         -1.7067, -0.3040,  0.8566, -1.1573,  0.0219, -0.5648, -0.2980, -0.8879],\n",
            "        [-0.2884, -0.5477,  1.5881, -0.5193, -0.1374, -0.5210,  0.9926,  2.0067,\n",
            "          0.1858,  0.6579, -0.6036, -0.9532, -0.2104,  0.8340,  0.9779, -0.4513],\n",
            "        [ 1.5774, -0.0252,  0.4255,  1.3154, -0.6366, -1.0235, -1.1520, -0.9622,\n",
            "          0.5502, -0.7158,  1.4739,  1.2995, -0.8994, -0.8676,  0.6065,  1.1507]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "‚úÖ After Positional Encoding:\n",
            " tensor([[ 0.4547,  0.1603, -0.5690, -0.3350, -1.1348,  0.9888,  0.1043,  1.8466,\n",
            "          0.4567, -0.4613, -1.2593,  1.4286, -0.8767,  1.0951, -0.4823, -0.1379],\n",
            "        [ 0.0186,  0.0542,  1.0782,  2.5620, -1.7943,  1.4144, -1.0893,  0.9605,\n",
            "          0.6238, -0.0110, -0.0114,  0.5444, -0.0071,  1.9316, -0.7011,  1.2700],\n",
            "        [ 0.6720,  0.2783,  0.3865,  3.0540, -1.0003,  1.0362, -0.5405,  1.1450,\n",
            "         -1.6867,  0.6958,  0.8630, -0.1573,  0.0239,  0.4352, -0.2974,  0.1121],\n",
            "        [-0.1473, -1.5377,  2.4007,  0.0635,  0.1581,  0.4344,  1.0873,  3.0022,\n",
            "          0.2158,  1.6574, -0.5942,  0.0467, -0.2074,  1.8340,  0.9789,  0.5487],\n",
            "        [ 0.8206, -0.6788,  1.3791,  1.6165, -0.2471, -0.1025, -1.0258,  0.0298,\n",
            "          0.5902,  0.2834,  1.4866,  2.2994, -0.8954,  0.1324,  0.6078,  2.1507]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "üöÄ Running Encoder Layer 1\n",
            "\n",
            "üß† === Transformer Encoder Layer ===\n",
            "\n",
            "üîç Attention Output (head 0):\n",
            " tensor([[ 0.1828,  0.4066, -0.2846,  0.5640],\n",
            "        [ 0.0996,  0.4077, -0.3436,  0.6616],\n",
            "        [ 0.1519,  0.4024, -0.3803,  0.6841],\n",
            "        [ 0.1673,  0.3889, -0.2723,  0.5371],\n",
            "        [ 0.1703,  0.3614, -0.2161,  0.4622]], grad_fn=<SelectBackward0>)\n",
            "\n",
            "‚öôÔ∏è Before FeedForward:\n",
            " tensor([[ 0.2942,  0.4967, -0.8741, -0.3838, -1.4094,  0.7625, -0.2695,  1.6275,\n",
            "          0.3283, -0.9033, -1.4393,  1.9579, -1.0785,  1.1367, -0.3989,  0.1530],\n",
            "        [-0.4677, -0.0804,  0.5345,  1.9963, -2.0570,  0.9757, -1.5841,  0.0435,\n",
            "          0.0775, -0.5591, -0.2140,  0.2456, -0.3991,  1.2610, -0.7833,  1.0106],\n",
            "        [ 0.2246,  0.2433, -0.0357,  2.9451, -1.2706,  0.3764, -0.9900,  0.3195,\n",
            "         -1.9093,  0.1903,  0.6426, -0.2594, -0.3059,  0.0173, -0.3218,  0.1337],\n",
            "        [-0.9684, -1.8631,  1.6997,  0.0480, -0.6291, -0.0860,  0.2075,  1.9145,\n",
            "         -0.6175,  0.8369, -1.0336, -0.3981, -0.9139,  1.0764,  0.4675,  0.2591],\n",
            "        [ 0.0957, -0.8036,  0.7623,  1.4472, -0.8032, -0.8012, -1.5615, -0.8243,\n",
            "         -0.0120, -0.3833,  1.0818,  1.6541, -1.2758, -0.3912,  0.2814,  1.5335]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "‚úÖ After FeedForward:\n",
            " tensor([[ 0.0486, -0.0184, -0.2745, -0.1876,  0.2308,  0.3336, -0.0925, -0.3304,\n",
            "          0.3604, -0.0408,  0.0138, -0.0519,  0.4737,  0.5367, -0.0494, -0.0463],\n",
            "        [ 0.0599, -0.0803,  0.0366,  0.0706, -0.0360,  0.2175, -0.1862, -0.7487,\n",
            "          0.6815,  0.0050, -0.3389, -0.0643,  0.7876,  0.8912, -0.5279,  0.3197],\n",
            "        [-0.2813, -0.5436, -0.0543, -0.0534,  0.3502, -0.0506, -0.2452, -0.7988,\n",
            "          0.8378,  0.0954, -0.4899, -0.0844,  0.7009,  0.5910, -0.5393,  0.0994],\n",
            "        [ 0.0534, -0.6923, -0.0572,  0.0073,  0.3483,  0.0808,  0.0870, -0.4996,\n",
            "          0.6263,  0.0589, -0.4808, -0.1000,  0.1870,  0.0246, -0.4825,  0.2924],\n",
            "        [ 0.4046,  0.2109, -0.4117, -0.1864, -0.2329,  0.4551,  0.0764, -0.3075,\n",
            "          0.3699, -0.1609,  0.1297, -0.2907,  0.6479,  0.5191,  0.1249,  0.2543]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "üöÄ Running Encoder Layer 2\n",
            "\n",
            "üß† === Transformer Encoder Layer ===\n",
            "\n",
            "üîç Attention Output (head 0):\n",
            " tensor([[-0.1140, -0.5226,  0.2495,  0.0701],\n",
            "        [-0.1682, -0.6182,  0.2643, -0.0904],\n",
            "        [-0.1832, -0.5961,  0.2441, -0.0784],\n",
            "        [-0.1361, -0.5754,  0.2616, -0.0468],\n",
            "        [-0.1665, -0.6102,  0.2740, -0.0992]], grad_fn=<SelectBackward0>)\n",
            "\n",
            "‚öôÔ∏è Before FeedForward:\n",
            " tensor([[ 0.3680,  0.3054, -1.2284, -0.5935, -1.3573,  0.9007, -0.2335,  1.2423,\n",
            "          0.5282, -0.7148, -1.4540,  1.6070, -0.6512,  1.8857, -0.2325, -0.3722],\n",
            "        [-0.3030, -0.3021, -0.2085,  1.5274, -1.9113,  0.8947, -1.3878, -0.5498,\n",
            "          0.5052, -0.3447, -0.5697,  0.4760,  0.3078,  2.1419, -0.8839,  0.6079],\n",
            "        [ 0.0520, -0.3978, -0.5070,  2.8074, -0.9081,  0.4150, -0.9954, -0.2933,\n",
            "         -1.9150,  0.5315,  0.1456,  0.1041,  0.5557,  1.0402, -0.4796, -0.1554],\n",
            "        [-0.6889, -2.4425,  0.9245, -0.0774, -0.2774, -0.0333,  0.5561,  1.5028,\n",
            "         -0.5145,  1.0890, -1.2633, -0.0126, -0.7224,  1.4062,  0.6974, -0.1436],\n",
            "        [ 0.5856, -0.7586, -0.4278,  0.9538, -1.4474, -0.5121, -1.4278, -1.4039,\n",
            "          0.3481, -0.4669,  1.2103,  1.7176, -0.7460,  0.4723,  0.5787,  1.3242]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "‚úÖ After FeedForward:\n",
            " tensor([[-0.3119,  0.6176,  0.0822,  0.0510,  0.0343, -0.1313,  0.3073, -0.2401,\n",
            "         -0.3694, -0.1634, -0.1078,  0.3434, -0.2665,  0.4534, -0.0115, -0.1008],\n",
            "        [-0.3349,  0.6000, -0.1878,  0.3744,  0.0409, -0.1478,  0.0336, -0.7003,\n",
            "          0.0352, -0.0455, -0.1638,  0.2670, -0.1197,  0.1321, -0.4074, -0.4477],\n",
            "        [ 0.1185,  0.8713, -0.2741,  0.6929,  0.1745, -0.4242,  0.2168, -0.6169,\n",
            "         -0.3399,  0.0913, -0.4997,  0.4570, -0.3559,  0.3284, -0.1843, -0.5554],\n",
            "        [-0.3322,  0.9697,  0.2910,  0.4802,  0.0146, -0.0941,  0.3121, -0.0700,\n",
            "         -0.4942, -0.2384, -0.4924,  0.3858, -0.1349,  0.8092,  0.0804, -0.3275],\n",
            "        [-0.6990,  0.6954,  0.3030,  0.3066,  0.0067, -0.0319, -0.0417, -0.4106,\n",
            "          0.2967, -0.5655,  0.0248,  0.1656, -0.1975,  0.5996,  0.0779, -0.5336]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "üì§ Final Output (Logits):\n",
            " tensor([[ 6.1321e-01,  5.2870e-01,  1.6087e-02, -5.5190e-01, -1.0038e+00,\n",
            "          8.4546e-01,  1.3598e-01, -1.0561e+00,  4.0361e-01,  4.5537e-01,\n",
            "         -8.4024e-02, -6.1050e-01,  1.6187e+00, -6.4220e-01, -6.1156e-02,\n",
            "         -1.1998e+00, -1.6306e-01, -6.9432e-01,  4.3158e-01, -9.0142e-03,\n",
            "          2.7932e-01,  5.1940e-02,  3.9019e-01,  3.5178e-01, -1.1463e+00,\n",
            "          5.6676e-01,  1.0852e-01,  8.2368e-01,  1.3577e+00,  5.5236e-01,\n",
            "          9.3053e-03, -7.6964e-01,  1.2399e-01, -4.4583e-01,  2.5799e-01,\n",
            "          6.1416e-01, -5.0422e-01,  1.0893e-01,  6.6422e-01, -1.5663e-02,\n",
            "         -8.6021e-01, -8.9173e-01,  4.5631e-01,  1.2665e-01, -6.4390e-01,\n",
            "          3.7973e-01, -3.4491e-01, -6.9844e-01, -1.0555e+00, -1.8762e-01],\n",
            "        [ 6.4895e-01,  4.1097e-01, -5.7705e-01, -3.1098e-02, -7.2321e-01,\n",
            "          2.7983e-01, -2.7631e-01, -9.9931e-01,  2.2014e-01,  1.1026e+00,\n",
            "          4.5803e-01,  1.8417e-01,  6.7893e-01, -7.6882e-01, -4.1729e-01,\n",
            "         -2.7086e-01, -4.0332e-02, -5.3378e-02, -1.2977e-01,  6.7550e-01,\n",
            "          2.0938e-01, -1.9130e-01,  3.0860e-01, -6.5597e-02, -2.9530e-01,\n",
            "          1.8503e-01,  1.3123e-01,  1.2915e-01,  8.1034e-01,  7.7791e-02,\n",
            "         -3.1543e-01, -6.0732e-02,  1.4655e+00, -4.0093e-01, -2.7952e-01,\n",
            "          2.6617e-01, -4.6558e-01, -2.6907e-02,  3.9604e-01, -6.1741e-01,\n",
            "         -8.0072e-01,  7.5652e-01,  6.6148e-01,  1.7443e-02, -2.2007e-01,\n",
            "          3.9539e-01, -2.7797e-01, -5.9067e-01, -1.0833e+00, -5.4232e-01],\n",
            "        [ 4.6075e-01, -4.3265e-04,  1.3521e-01, -1.0809e-01, -8.9255e-02,\n",
            "          3.8248e-01,  5.6884e-01,  2.3396e-01, -3.2028e-01,  1.1629e+00,\n",
            "          4.3581e-01,  2.6143e-01,  2.5706e-01, -5.1491e-01, -8.2143e-01,\n",
            "          2.1884e-01,  1.4444e-01, -2.7601e-01,  5.3621e-01,  4.1324e-01,\n",
            "         -6.2548e-01, -7.7250e-01, -1.9113e-01,  1.7197e-01,  2.0508e-01,\n",
            "          4.4997e-01, -3.0475e-02, -6.2155e-01, -1.5750e-02,  1.1934e+00,\n",
            "          1.5334e-01, -3.8216e-01,  1.0493e+00,  2.4918e-02, -2.4179e-01,\n",
            "          2.2732e-01,  3.4140e-01, -4.2877e-01, -6.8932e-01,  4.7951e-03,\n",
            "         -4.8230e-01,  8.5818e-01, -1.1991e-01,  1.8979e-01,  1.8551e-01,\n",
            "          6.2768e-01, -4.1592e-01,  2.8465e-01, -4.7745e-01,  4.6716e-01],\n",
            "        [ 8.7931e-01, -4.6890e-01,  4.6061e-01, -6.3108e-01, -3.6295e-01,\n",
            "          1.2632e-02, -1.6089e-01,  6.4807e-01,  5.9726e-01,  8.7802e-01,\n",
            "          5.6714e-02, -7.3849e-01,  1.2203e+00,  1.7592e-01, -1.5722e-01,\n",
            "         -6.2455e-01, -3.6586e-01, -8.8327e-01,  8.3489e-01, -1.6552e-01,\n",
            "          8.0162e-01, -6.3246e-01, -6.3480e-01, -1.5809e-01, -9.4118e-01,\n",
            "          6.8732e-01, -4.0319e-01, -4.9894e-01,  1.0242e+00,  8.7900e-01,\n",
            "         -7.5073e-01,  2.5925e-01, -4.9450e-01, -5.3492e-02, -1.3623e+00,\n",
            "         -9.3177e-03,  9.1479e-01, -5.0793e-02,  5.1976e-01, -5.1060e-01,\n",
            "         -1.8569e-01,  3.6624e-01,  8.0297e-02, -7.7668e-02, -2.0775e-01,\n",
            "          9.2507e-01, -8.0252e-01,  2.9562e-01, -1.1828e-01,  7.0429e-01],\n",
            "        [-2.3693e-01,  8.5689e-01, -3.9541e-01,  1.5424e-01, -1.1827e+00,\n",
            "          4.4998e-01, -4.8955e-01, -5.1468e-02, -6.0718e-01,  1.2273e+00,\n",
            "          7.0657e-01,  1.1318e-01,  6.1811e-02, -4.2405e-01,  4.3939e-01,\n",
            "         -2.4837e-01, -2.4793e-01, -5.0961e-01, -4.7016e-01,  7.3413e-01,\n",
            "          4.7781e-01, -3.3438e-01,  6.0347e-01, -5.3642e-01,  1.4158e-01,\n",
            "          2.8072e-02,  2.6803e-02, -1.9580e-01,  6.6466e-01, -2.4948e-01,\n",
            "         -3.4521e-01,  1.1741e-01,  1.1830e+00, -4.5186e-01, -8.3077e-02,\n",
            "          5.1845e-01, -1.0419e-01, -2.4742e-01,  1.1347e-01, -1.4664e-01,\n",
            "         -2.9764e-01,  2.0783e-01,  1.1117e+00, -2.2150e-01,  2.8330e-02,\n",
            "         -6.7941e-01, -3.0808e-01, -8.0235e-01, -7.5146e-01, -6.7099e-01]],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BbfGh04ETgZx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}