{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP  Assignment No. 1"
      ],
      "metadata": {
        "id": "HcYhtBUmdbw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Title :Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using NLTK library. Use porter stemmer and snowball stemmer for stemming. Use any technique for lemmatization."
      ],
      "metadata": {
        "id": "9jDa0l8-dmQB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghSVZyh9bHeg"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, TreebankWordTokenizer, TweetTokenizer, MWETokenizer, WhitespaceTokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download everything (big but effective)\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRrPOtHdcRbD",
        "outputId": "29a56fef-5864-43e6-c03a-3984f060af8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sentence\n",
        "sentence = \"I can't believe it's already 2025! Time flies fast, doesn't it? #life #tech\"\n",
        "\n",
        "print(\"Original Sentence:\")\n",
        "print(sentence)\n",
        "print(\"\\n--- Tokenization ---\\n\")\n",
        "print(\"\\n--- Stemming ---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjmqZRqObqar",
        "outputId": "7441e6ee-4140-4f5e-9436-8c9826f56fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence:\n",
            "I can't believe it's already 2025! Time flies fast, doesn't it? #life #tech\n",
            "\n",
            "--- Tokenization ---\n",
            "\n",
            "\n",
            "--- Stemming ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization:"
      ],
      "metadata": {
        "id": "6U5V3WGgdYkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Whitespace Tokenizer\n",
        "whitespace_tokens = WhitespaceTokenizer().tokenize(sentence)\n",
        "print(\"Whitespace Tokenizer:\", whitespace_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDM9Hzs_cYbp",
        "outputId": "e8ae9d21-652e-4497-d052-c844cd5ef855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenizer: ['I', \"can't\", 'believe', \"it's\", 'already', '2025!', 'Time', 'flies', 'fast,', \"doesn't\", 'it?', '#life', '#tech']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Punctuation-based Tokenizer\n",
        "punct_tokens = [word.strip(string.punctuation) for word in sentence.split()]\n",
        "punct_tokens = [word for word in punct_tokens if word]  # Remove empty tokens\n",
        "print(\"Punctuation-based Tokenizer:\", punct_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXcu5cuycd5B",
        "outputId": "fef8e7ad-4fe8-4dc7-bbba-f7438d17cb19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation-based Tokenizer: ['I', \"can't\", 'believe', \"it's\", 'already', '2025', 'Time', 'flies', 'fast', \"doesn't\", 'it', 'life', 'tech']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Treebank Tokenizer\n",
        "treebank_tokens = TreebankWordTokenizer().tokenize(sentence)\n",
        "print(\"Treebank Tokenizer:\", treebank_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGVgk30qcd8q",
        "outputId": "de996f06-59db-4ac8-caeb-e43102ee98b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank Tokenizer: ['I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'already', '2025', '!', 'Time', 'flies', 'fast', ',', 'does', \"n't\", 'it', '?', '#', 'life', '#', 'tech']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Tweet Tokenizer\n",
        "tweet_tokens = TweetTokenizer().tokenize(sentence)\n",
        "print(\"Tweet Tokenizer:\", tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gtIKKQScd_7",
        "outputId": "c75d38b5-ecf8-4ec0-ec42-bcf1125723d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet Tokenizer: ['I', \"can't\", 'believe', \"it's\", 'already', '2025', '!', 'Time', 'flies', 'fast', ',', \"doesn't\", 'it', '?', '#life', '#tech']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. MWE Tokenizer\n",
        "mwe_tokenizer = MWETokenizer([('New', 'York'), (\"don't\", 'know')], separator='_')\n",
        "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(sentence))\n",
        "print(\"MWE Tokenizer:\", mwe_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRG1WBm2csaj",
        "outputId": "e344e766-1382-4e5d-9399-e315315b8f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MWE Tokenizer: ['I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'already', '2025', '!', 'Time', 'flies', 'fast', ',', 'does', \"n't\", 'it', '?', '#', 'life', '#', 'tech']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming:"
      ],
      "metadata": {
        "id": "1bSUNON6c-Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "porter_stems = [porter.stem(token) for token in word_tokenize(sentence)]\n",
        "print(\"Porter Stemmer:\", porter_stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1-TrQg1bqdp",
        "outputId": "878cdb36-b6a8-4f7a-dbcd-48a7bde954b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer: ['i', 'ca', \"n't\", 'believ', 'it', \"'s\", 'alreadi', '2025', '!', 'time', 'fli', 'fast', ',', 'doe', \"n't\", 'it', '?', '#', 'life', '#', 'tech']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Snowball Stemmer\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "snowball_stems = [snowball.stem(token) for token in word_tokenize(sentence)]\n",
        "print(\"Snowball Stemmer:\", snowball_stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "032niUsjbqg5",
        "outputId": "7e2b7cd9-5bd8-4829-8b74-da1bc2bda608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snowball Stemmer: ['i', 'ca', \"n't\", 'believ', 'it', \"'s\", 'alreadi', '2025', '!', 'time', 'fli', 'fast', ',', 'doe', \"n't\", 'it', '?', '#', 'life', '#', 'tech']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization"
      ],
      "metadata": {
        "id": "2l_vsq1WdTw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Lemmatization ---\\n\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(token.lower()) for token in word_tokenize(sentence)]\n",
        "print(\"Lemmatized Words:\", lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1VD5Lu1c4ih",
        "outputId": "f15584e0-04c4-4d12-ed62-5a46a4c0e2e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Lemmatization ---\n",
            "\n",
            "Lemmatized Words: ['i', 'ca', \"n't\", 'believe', 'it', \"'s\", 'already', '2025', '!', 'time', 'fly', 'fast', ',', 'doe', \"n't\", 'it', '?', '#', 'life', '#', 'tech']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Completed**"
      ],
      "metadata": {
        "id": "-kgd_U6LfPhi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KCm8QeItfT_h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}